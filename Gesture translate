Gesture translate

import cv2
import mediapipe as mp
import pandas as pd
import numpy as np
import time  # For delay functionality

data = pd.read_csv("gesture_data.csv")

X = data.iloc[:, :-1].values  
actions = data.iloc[:, -1].values  

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

# Open the webcam
cap = cv2.VideoCapture(0)

print("Show a gesture to match with stored data. Press 'q' to quit.")

last_detected_time = time.time()  # Time when the last action was detected

with mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7) as hands:
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            print("Failed to access webcam.")
            break

        frame = cv2.flip(frame, 1)
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        results = hands.process(rgb_frame)

        matched_action = None

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # Draw landmarks on the frame
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                # Extract keypoints from the detected hand
                keypoints = []
                for landmark in hand_landmarks.landmark:
                    keypoints.extend([landmark.x, landmark.y, landmark.z])

                # Convert keypoints to a NumPy array for comparison
                keypoints = np.array(keypoints).reshape(1, -1)

                # Check if keypoints array size matches stored keypoints
                if keypoints.shape[1] != X.shape[1]:
                    print("Detected hand landmarks do not match stored data dimensions.")
                    continue

                # Compare with stored gestures
                for i, stored_keypoints in enumerate(X):
                    if np.allclose(stored_keypoints, keypoints, atol=0.18):  # Adjust tolerance as needed
                        matched_action = actions[i]
                        break

        current_time = time.time()
        if matched_action and (current_time - last_detected_time) > 2:
            print(matched_action, end=" ", flush=True)  # Print on the same line
            last_detected_time = current_time  # Update the time of detection

        # Display the matched action on the video feed
        display_text = matched_action if matched_action else "No Match Found"
        cv2.putText(frame, f"Action: {display_text}", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        # Show the video feed
        cv2.imshow("Sign Language Recognition", frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

# Release resources
cap.release()
cv2.destroyAllWindows()
